{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "from IPython.display import clear_output\n",
    "print(platform.platform())\n",
    "\n",
    "def resolve_dir(Dir):\n",
    "    if not os.path.exists(Dir):\n",
    "        os.mkdir(Dir)\n",
    "\n",
    "def reset_path(Dir):\n",
    "    if not os.path.exists(Dir):\n",
    "        os.mkdir(Dir)\n",
    "    else:\n",
    "        os.system('rm -f {}/*'.format( Dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(73)\n",
    "TPU_INIT = False\n",
    "\n",
    "if TPU_INIT:\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
    "        tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    \n",
    "    except ValueError:\n",
    "        raise BaseException('ERROR: Not connected to a TPU runtime!')\n",
    "else:\n",
    "    !nvidia-smi\n",
    ";    \n",
    "print(\"Tensorflow version \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyDrive = 'https://drive.google.com/drive/u/0/folders/1fbuphoJZO_15KAeDyyeJswFrRhHDUMWY'\n",
    "PROJECT_DIR = './Downloads/violencedataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing\n",
    "Getting frames form video\n",
    "some image argumentations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import imageio\n",
    "import imgaug.augmenters as iaa\n",
    "import imgaug as ia\n",
    "\n",
    "IMG_SIZE = 128\n",
    "ColorChannels = 3\n",
    "\n",
    "def video_to_frames(video):\n",
    "    vidcap = cv2.VideoCapture(video)\n",
    "    \n",
    "    import math\n",
    "    rate = math.floor(vidcap.get(3))\n",
    "    count = 0\n",
    "    \n",
    "    ImageFrames = []\n",
    "    while vidcap.isOpened():\n",
    "        ID = vidcap.get(1)\n",
    "        success, image = vidcap.read()\n",
    "        \n",
    "        if success:\n",
    "            # skipping frames to avoid duplications \n",
    "            if (ID % 7 == 0):\n",
    "                flip = iaa.Fliplr(1.0)\n",
    "                zoom = iaa.Affine(scale=1.3)\n",
    "                random_brightness = iaa.Multiply((1, 1.3))\n",
    "                rotate = iaa.Affine(rotate=(-25, 25))\n",
    "                \n",
    "                image_aug = flip(image = image)\n",
    "                image_aug = random_brightness(image = image_aug)\n",
    "                image_aug = zoom(image = image_aug)\n",
    "                image_aug = rotate(image = image_aug)\n",
    "                \n",
    "                rgb_img = cv2.cvtColor(image_aug, cv2.COLOR_BGR2RGB)\n",
    "                resized = cv2.resize(rgb_img, (IMG_SIZE, IMG_SIZE))\n",
    "                ImageFrames.append(resized)\n",
    "                \n",
    "            count += 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    vidcap.release()\n",
    "    \n",
    "    return ImageFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from tqdm import tqdm\n",
    "\n",
    "VideoDataDir = PROJECT_DIR + '/Real Life Violence Dataset'\n",
    "print('we have \\n{} Violence videos \\n{} NonViolence videos'.format(\n",
    "              len(os.listdir(VideoDataDir + '/Violence')), \n",
    "              len(os.listdir(VideoDataDir + '/NonViolence'))))\n",
    "\n",
    "X_original = []\n",
    "y_original = []\n",
    "\n",
    "print('i choose 700 videos out of 2000, cuz of memory issue')\n",
    "CLASSES = [\"NonViolence\", \"Violence\"]\n",
    "#700 <- 350 + 350\n",
    "\n",
    "for category in os.listdir(VideoDataDir):\n",
    "    path = os.path.join(VideoDataDir, category)\n",
    "    class_num = CLASSES.index(category)\n",
    "    for i, video in enumerate(tqdm(os.listdir(path)[0:350])):\n",
    "        frames = video_to_frames(path + '/' + video)\n",
    "        for j, frame in enumerate(frames):\n",
    "            X_original.append(frame)\n",
    "            y_original.append(class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_original = np.array(X_original).reshape(-1 , IMG_SIZE * IMG_SIZE * 3)\n",
    "y_original = np.array(y_original)\n",
    "len(X_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_original' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StratifiedShuffleSplit\n\u001b[0;32m      3\u001b[0m stratified_sample \u001b[38;5;241m=\u001b[39m StratifiedShuffleSplit(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m73\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_index, test_index \u001b[38;5;129;01min\u001b[39;00m stratified_sample\u001b[38;5;241m.\u001b[39msplit(\u001b[43mX_original\u001b[49m, y_original):\n\u001b[0;32m      6\u001b[0m     X_train, X_test \u001b[38;5;241m=\u001b[39m X_original[train_index], X_original[test_index]\n\u001b[0;32m      7\u001b[0m     y_train, y_test \u001b[38;5;241m=\u001b[39m y_original[train_index], y_original[test_index]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_original' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "stratified_sample = StratifiedShuffleSplit(n_splits=2, test_size=0.3, random_state=73)\n",
    "\n",
    "for train_index, test_index in stratified_sample.split(X_original, y_original):\n",
    "    X_train, X_test = X_original[train_index], X_original[test_index]\n",
    "    y_train, y_test = y_original[train_index], y_original[test_index]\n",
    "\n",
    "X_train_nn = X_train.reshape(-1, IMG_SIZE, IMG_SIZE, 3) / 255\n",
    "X_test_nn = X_test.reshape(-1, IMG_SIZE, IMG_SIZE, 3) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dropout,Flatten,Dense\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "\n",
    "from keras import regularizers\n",
    "kernel_regularizer = regularizers.l2(0.0001)\n",
    "\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "\n",
    "def load_layers():\n",
    "    input_tensor = Input(shape=(IMG_SIZE, IMG_SIZE, ColorChannels))\n",
    "    baseModel = MobileNetV2(pooling='avg',\n",
    "                            include_top=False, \n",
    "                            input_tensor=input_tensor)\n",
    "    \n",
    "    headModel = baseModel.output   \n",
    "    headModel = Dense(1, activation=\"sigmoid\")(headModel)\n",
    "    model = Model(inputs=baseModel.input, outputs=headModel)\n",
    "\n",
    "    for layer in baseModel.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    print(\"Compiling model...\")\n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "                    optimizer='adam',\n",
    "                    metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "if TPU_INIT:\n",
    "    with tpu_strategy.scope():\n",
    "        model = load_layers()\n",
    "else:\n",
    "    model = load_layers()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "patience = 3\n",
    "\n",
    "start_lr = 0.00001\n",
    "min_lr = 0.00001\n",
    "max_lr = 0.00005\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "if TPU_INIT:\n",
    "    max_lr = max_lr * tpu_strategy.num_replicas_in_sync\n",
    "    batch_size = batch_size * tpu_strategy.num_replicas_in_sync\n",
    "\n",
    "rampup_epochs = 5\n",
    "sustain_epochs = 0\n",
    "exp_decay = .8\n",
    "\n",
    "def lrfn(epoch):\n",
    "    if epoch < rampup_epochs:\n",
    "        return (max_lr - start_lr)/rampup_epochs * epoch + start_lr\n",
    "    elif epoch < rampup_epochs + sustain_epochs:\n",
    "        return max_lr\n",
    "    else:\n",
    "        return (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr\n",
    "\n",
    "\n",
    "class myCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if ((logs.get('accuracy')>=0.999)):\n",
    "            print(\"\\nLimits Reached cancelling training!\")\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_callback = myCallback()\n",
    "\n",
    "lr_callback = LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=False)\n",
    "\n",
    "early_stopping = EarlyStopping(patience = patience, monitor='val_loss',\n",
    "                                 mode='min', restore_best_weights=True, \n",
    "                                 verbose = 1, min_delta = .00075)\n",
    "\n",
    "PROJECT_DIR = MyDrive + '/RiskDetection'\n",
    "\n",
    "lr_plat = ReduceLROnPlateau(patience = 2, mode = 'min')\n",
    "\n",
    "os.system('rm -rf ./logs/')\n",
    "\n",
    "import datetime\n",
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir = log_dir, write_graph=True, histogram_freq=1)\n",
    "\n",
    "checkpoint_filepath = 'ModelWeights.h5'\n",
    "\n",
    "model_checkpoints = ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                                        save_weights_only=True,\n",
    "                                        monitor='val_loss',\n",
    "                                        mode='min',\n",
    "                                        verbose = 1,\n",
    "                                        save_best_only=True)\n",
    "\n",
    "\n",
    "callbacks = [end_callback, lr_callback, model_checkpoints, tensorboard_callback, early_stopping, lr_plat]\n",
    "\n",
    "if TPU_INIT:\n",
    "    callbacks = [end_callback, lr_callback, model_checkpoints, early_stopping, lr_plat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training head...')\n",
    "#model.load_weights('./Model_Weights.h5')\n",
    "\n",
    "history = model.fit(X_train_nn ,y_train, epochs=epochs,\n",
    "                        callbacks=callbacks,\n",
    "                        validation_data = (X_test_nn, y_test),\n",
    "                        batch_size=batch_size)\n",
    "\n",
    "print('\\nRestoring best Weights for MobileNetV2')\n",
    "model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def print_graph(item, index, history):\n",
    "    plt.figure()\n",
    "    train_values = history.history[item][0:index]\n",
    "    plt.plot(train_values)\n",
    "    test_values = history.history['val_' + item][0:index]\n",
    "    plt.plot(test_values)\n",
    "    plt.legend(['training','validation'])\n",
    "    plt.title('Training and validation '+ item)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "    plot = '{}.png'.format(item)\n",
    "    plt.savefig(plot)\n",
    "\n",
    "\n",
    "def get_best_epoch(test_loss, history):\n",
    "    for key, item in enumerate(history.history.items()):\n",
    "        (name, arr) = item\n",
    "        if name == 'val_loss':\n",
    "            for i in range(len(arr)):\n",
    "                if round(test_loss, 2) == round(arr[i], 2):\n",
    "                    return i\n",
    "                \n",
    "def model_summary(model, history):\n",
    "    print('---'*30)\n",
    "    test_loss, test_accuracy = model.evaluate(X_test_nn, y_test, verbose=0)\n",
    "\n",
    "    if history:\n",
    "        index = get_best_epoch(test_loss, history)\n",
    "        print('Best Epochs: ', index)\n",
    "\n",
    "        train_accuracy = history.history['accuracy'][index]\n",
    "        train_loss = history.history['loss'][index]\n",
    "\n",
    "        print('Accuracy on train:',train_accuracy,'\\tLoss on train:',train_loss)\n",
    "        print('Accuracy on test:',test_accuracy,'\\tLoss on test:',test_loss)\n",
    "        print_graph('loss', index, history)\n",
    "        print_graph('accuracy', index, history)\n",
    "        print('---'*30)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the network\n",
    "print(\"Evaluating network...\")\n",
    "predictions = model.predict(X_test_nn)\n",
    "preds = predictions > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: '_pydevd_frame_eval.pydevd_frame_evaluator.get_bytecode_while_frame_eval_39'\n",
      "Traceback (most recent call last):\n",
      "  File \"_pydevd_frame_eval/pydevd_frame_evaluator.pyx\", line 258, in _pydevd_frame_eval.pydevd_frame_evaluator.get_func_code_info\n",
      "  File \"C:\\Users\\HP\\AppData\\Roaming\\Python\\Python310\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd_file_utils.py\", line 885, in get_abs_path_real_path_and_base_from_frame\n",
      "    ret = get_abs_path_real_path_and_base_from_file(f)\n",
      "  File \"C:\\Users\\HP\\AppData\\Roaming\\Python\\Python310\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd_file_utils.py\", line 852, in get_abs_path_real_path_and_base_from_file\n",
      "    abs_path, canonical_normalized_filename = _abs_and_canonical_path(f)\n",
      "  File \"C:\\Users\\HP\\AppData\\Roaming\\Python\\Python310\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd_file_utils.py\", line 411, in _abs_and_canonical_path\n",
      "    real_path = _apply_func_and_normalize_case(filename, os_path_real_path, isabs, normalize)\n",
      "  File \"C:\\Users\\HP\\AppData\\Roaming\\Python\\Python310\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd_file_utils.py\", line 438, in _apply_func_and_normalize_case\n",
      "    r = func(filename)\n",
      "  File \"c:\\Program Files\\Python310\\lib\\ntpath.py\", line 660, in realpath\n",
      "    path = _getfinalpathname(path)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\HP\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3378, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_23020\\3180812836.py\", line 1, in <module>\n",
      "    import seaborn as sns\n",
      "ModuleNotFoundError: No module named 'seaborn'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\HP\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 1997, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"C:\\Users\\HP\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 1112, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"C:\\Users\\HP\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 1006, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"C:\\Users\\HP\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 859, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"C:\\Users\\HP\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 791, in format_exception_as_a_whole\n",
      "    head = self.prepare_header(etype, self.long_header)\n",
      "  File \"C:\\Users\\HP\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 737, in prepare_header\n",
      "    width = min(75, get_terminal_size()[0])\n",
      "  File \"C:\\Users\\HP\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\utils\\terminal.py\", line 129, in get_terminal_size\n",
      "    return _get_terminal_size((defaultx, defaulty))\n",
      "SystemError: <function get_terminal_size at 0x00000282B4D73D90> returned NULL without setting an exception\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, plot_roc_curve, accuracy_score, classification_report, confusion_matrix\n",
    "corr_pred = metrics.confusion_matrix(y_test, preds)\n",
    "\n",
    "n_correct = np.int((corr_pred[0][0] + corr_pred[1][1]))\n",
    "print('> Correct Predictions:', n_correct)\n",
    "n_wrongs = np.int((corr_pred[0][1] + (corr_pred[1][0])))\n",
    "print('> Wrong Predictions:', n_wrongs)\n",
    "\n",
    "sns.heatmap(corr_pred,annot=True, fmt=\"d\",cmap=\"Blues\")\n",
    "plt.show()\n",
    "\n",
    "print(metrics.classification_report(y_test, preds, \n",
    "                           target_names=[\"NonViolence\", \"Violence\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_model = \"modelnew.h5\"\n",
    "model.save(args_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
